"""
åˆ†ç±»/èšç±» Agent

èŒè´£ï¼šå¯¹é•¿ä¸Šä¸‹æ–‡æŒ‰ä¸»é¢˜è¿›è¡Œåˆ†ç±»/èšç±»

å‚è€ƒï¼šè§„èŒƒæ–‡æ¡£ç¬¬5.1èŠ‚
"""

import json
import logging
from typing import Optional, List
from dataclasses import dataclass
from src.agents.base_agent import BaseAgent
from src.prompts.agent_prompts import CLASSIFICATION_PROMPT

logger = logging.getLogger(__name__)


@dataclass
class ClassificationInput:
    """åˆ†ç±» Agent è¾“å…¥"""
    context: str  # é•¿ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼ˆå¯èƒ½è¶…é•¿ï¼‰
    task_goal: Optional[str] = None  # æ€»ä»»åŠ¡ç›®æ ‡ï¼ˆå‚è€ƒï¼‰
    current_task: Optional[str] = None  # å½“å‰å­ä»»åŠ¡ï¼ˆå‚è€ƒï¼Œå¸®åŠ©è¯†åˆ«é‡è¦ä¿¡æ¯ï¼‰


@dataclass
class Cluster:
    """èšç±»ç»“æœ"""
    cluster_id: str  # èšç±»ID
    context: str  # ä¸€å¥è¯ä¸»é¢˜æè¿°
    content: str  # å±äºè¯¥ä¸»é¢˜çš„åŸå§‹æ–‡æœ¬å†…å®¹
    keywords: List[str]  # å…³é”®è¯åˆ—è¡¨


@dataclass
class ClassificationOutput:
    """åˆ†ç±» Agent è¾“å‡º"""
    should_cluster: bool  # æ˜¯å¦éœ€è¦èšç±»
    clusters: List[Cluster]  # èšç±»åˆ—è¡¨


class ClassificationAgent(BaseAgent):
    """
    åˆ†ç±»/èšç±» Agent

    å¤„ç†è¶…é•¿æ–‡æœ¬æ—¶ä½¿ç”¨åˆ†å—ç­–ç•¥
    """

    def __init__(self, llm_client, window_size: int = 8000, chunk_ratio: float = 0.9,
                 temperature: float = 0.4, top_p: float = 0.9):
        """
        åˆå§‹åŒ–åˆ†ç±» Agent

        Args:
            llm_client: LLMClient å®ä¾‹
            window_size: Agent çª—å£å¤§å°ï¼ˆtokenï¼‰
            chunk_ratio: åˆ†å—æ¯”ä¾‹ï¼ˆç•™ä½™é‡ï¼‰
            temperature: æ¸©åº¦å‚æ•°
            top_p: é‡‡æ ·å‚æ•°
        """
        super().__init__(llm_client)
        self.window_size = window_size
        self.chunk_ratio = chunk_ratio
        self.temperature = temperature
        self.top_p = top_p
        logger.info(f"åˆ†ç±»Agentåˆå§‹åŒ–: window_size={window_size}, chunk_ratio={chunk_ratio}, "
                   f"temp={temperature}, top_p={top_p}")

    @classmethod
    def from_config(cls, llm_client, config) -> "ClassificationAgent":
        """ä»é…ç½®åˆ›å»ºAgent"""
        return cls(
            llm_client=llm_client,
            window_size=config.CLASSIFICATION_AGENT_WINDOW,
            chunk_ratio=config.CHUNK_RATIO,
            temperature=config.CLASSIFICATION_AGENT_TEMPERATURE,
            top_p=config.CLASSIFICATION_AGENT_TOP_P
        )

    def run(self, input_data: ClassificationInput) -> ClassificationOutput:
        """
        æ‰§è¡Œåˆ†ç±»/èšç±»

        Args:
            input_data: ClassificationInput å®ä¾‹

        Returns:
            ClassificationOutput å®ä¾‹
        """
        # 1. æ£€æŸ¥æ˜¯å¦è¶…é•¿
        token_count = self.llm_client.count_tokens(input_data.context)

        if token_count <= self.window_size:
            # ä¸è¶…é•¿ï¼Œç›´æ¥è°ƒç”¨ LLM
            return self._classify_single_chunk(input_data)
        else:
            # è¶…é•¿ï¼Œåˆ†æ¬¡åŠ è½½
            logger.warning(f"ä¸Šä¸‹æ–‡è¶…é•¿ ({token_count} tokens)ï¼Œå¯ç”¨åˆ†å—å¤„ç†")
            return self._classify_multiple_chunks(input_data)

    def _classify_single_chunk(self, input_data: ClassificationInput) -> ClassificationOutput:
        """
        å¤„ç†å•ä¸ªå—

        Args:
            input_data: ClassificationInput å®ä¾‹

        Returns:
            ClassificationOutput å®ä¾‹
        """
        prompt = self._build_prompt(input_data.context, input_data.task_goal, input_data.current_task)

        logger.debug(f"è°ƒç”¨LLMè¿›è¡Œåˆ†ç±» (temp={self.temperature}, top_p={self.top_p})...")
        response = self.llm_client.call(prompt, temperature=self.temperature, top_p=self.top_p)

        # è®°å½•LLMåŸå§‹å“åº”
        logger.debug("="*80)
        logger.debug("ğŸ“¤ Classification Agent LLMåŸå§‹å“åº”:")
        logger.debug(response)
        logger.debug("="*80)

        return self._parse_response(response, input_data)

    def _classify_multiple_chunks(self, input_data: ClassificationInput) -> ClassificationOutput:
        """
        å¤„ç†å¤šä¸ªå—ï¼ˆåˆ†æ¬¡åŠ è½½ï¼‰

        Args:
            input_data: ClassificationInput å®ä¾‹

        Returns:
            ClassificationOutput å®ä¾‹
        """
        chunk_size = int(self.window_size * self.chunk_ratio)
        chunks = self._split_by_boundaries(input_data.context, chunk_size)

        logger.info(f"ä¸Šä¸‹æ–‡åˆ†ä¸º {len(chunks)} ä¸ªå—")

        all_clusters = []
        for i, chunk in enumerate(chunks, 1):
            logger.debug(f"å¤„ç†å— {i}/{len(chunks)}")
            chunk_input = ClassificationInput(
                context=chunk,
                task_goal=input_data.task_goal
            )
            chunk_output = self._classify_single_chunk(chunk_input)
            all_clusters.extend(chunk_output.clusters)

        return ClassificationOutput(should_cluster=True, clusters=all_clusters)

    def _build_prompt(self, context: str, task_goal: Optional[str], current_task: Optional[str]) -> str:
        """
        æ„å»º prompt

        Args:
            context: ä¸Šä¸‹æ–‡å†…å®¹
            task_goal: æ€»ä»»åŠ¡ç›®æ ‡ï¼ˆå¯é€‰ï¼‰
            current_task: å½“å‰å­ä»»åŠ¡ï¼ˆå¯é€‰ï¼‰

        Returns:
            å®Œæ•´ prompt
        """
        return CLASSIFICATION_PROMPT.format(
            task_goal=task_goal or "ï¼ˆæ— ï¼‰",
            current_task=current_task or "ï¼ˆæ— ï¼‰",
            context=context
        )

    def _parse_response(self, response: str, input_data: ClassificationInput = None) -> ClassificationOutput:
        """
        è§£æ LLM å“åº”ï¼ˆç®€å•åˆ†éš”ç¬¦æ ¼å¼ï¼ŒéJSONï¼‰

        Args:
            response: LLM å“åº”å­—ç¬¦ä¸²
            input_data: ClassificationInput å®ä¾‹ï¼ˆç”¨äºå¡«å……contentï¼‰

        Returns:
            ClassificationOutput å®ä¾‹
        """
        try:
            # è§£æ SHOULD_CLUSTER
            should_cluster = False
            if "SHOULD_CLUSTER:" in response:
                should_cluster_line = [line for line in response.split('\n') if 'SHOULD_CLUSTER:' in line][0]
                should_cluster = 'true' in should_cluster_line.lower()

            # æŒ‰ === CLUSTER åˆ†éš”ç¬¦æ‹†åˆ†
            cluster_blocks = response.split('=== CLUSTER')[1:]  # è·³è¿‡ç¬¬ä¸€éƒ¨åˆ†ï¼ˆSHOULD_CLUSTERè¡Œï¼‰

            clusters = []
            for i, block in enumerate(cluster_blocks, 1):
                try:
                    # æå–cluster_idï¼ˆä» "c1 ===" æˆ– "c2 ===" ä¸­æå–ï¼‰
                    cluster_id_match = block.split('===')[0].strip()
                    cluster_id = cluster_id_match if cluster_id_match else f"c{i}"

                    # æå– CONTEXT
                    context = ""
                    if "CONTEXT:" in block:
                        context_line = [line for line in block.split('\n') if line.strip().startswith('CONTEXT:')][0]
                        context = context_line.split('CONTEXT:', 1)[1].strip()

                    # æå– KEYWORDS
                    keywords = []
                    if "KEYWORDS:" in block:
                        keywords_line = [line for line in block.split('\n') if line.strip().startswith('KEYWORDS:')][0]
                        keywords_str = keywords_line.split('KEYWORDS:', 1)[1].strip()
                        keywords = [kw.strip() for kw in keywords_str.split(',')]

                    # âœ… ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨åŸå§‹è¾“å…¥ä½œä¸ºcontentï¼Œè€Œä¸æ˜¯è®©LLMå¤åˆ¶
                    # é¿å…æµªè´¹tokenå’ŒLLMå¯èƒ½çš„å¤åˆ¶é”™è¯¯
                    content = input_data.context if input_data else ""

                    if not content:
                        logger.warning(f"cluster {i} ç¼ºå°‘contentï¼ˆinput_dataä¸ºç©ºï¼‰ï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²")

                    # åˆ›å»º Cluster å¯¹è±¡
                    cluster = Cluster(
                        cluster_id=cluster_id,
                        context=context,
                        content=content,
                        keywords=keywords
                    )
                    clusters.append(cluster)

                except Exception as e:
                    logger.warning(f"è§£æclusterå— {i} å¤±è´¥: {str(e)}, è·³è¿‡è¯¥å—")
                    continue

            # å¦‚æœæˆåŠŸè§£æåˆ°clusterï¼Œè¿”å›ç»“æœ
            if clusters:
                return ClassificationOutput(
                    should_cluster=should_cluster,
                    clusters=clusters
                )
            else:
                raise ValueError("æœªèƒ½è§£æå‡ºä»»ä½•cluster")

        except Exception as e:
            logger.error(f"è§£æåˆ†ç±»å“åº”å¤±è´¥: {str(e)}")
            logger.debug(f"å“åº”å†…å®¹ï¼ˆå‰1000å­—ç¬¦ï¼‰: {response[:1000]}")

            # Fallback: è¿”å›é»˜è®¤å•ä¸€cluster
            import re

            # å¦‚æœæœ‰input_dataï¼Œä»åŸå§‹contextä¸­æå–å…³é”®è¯
            if input_data and input_data.context:
                context_preview = input_data.context[:100] + "..." if len(input_data.context) > 100 else input_data.context
                content_fallback = input_data.context
            else:
                context_preview = "è§£æå¤±è´¥çš„é»˜è®¤åˆ†ç±»"
                content_fallback = response[:500]

            # ä»åŸå§‹contextä¸­æå–ä¸­æ–‡è¯ç»„å’Œè‹±æ–‡å•è¯ä½œä¸ºå…³é”®è¯
            words = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]{3,}', context_preview)
            fallback_keywords = list(set(words[:5])) if words else ["é»˜è®¤åˆ†ç±»"]

            return ClassificationOutput(
                should_cluster=False,
                clusters=[Cluster(
                    cluster_id="c1",
                    context=context_preview,
                    content=content_fallback,
                    keywords=fallback_keywords
                )]
            )

    def _split_by_boundaries(self, text: str, chunk_size: int) -> List[str]:
        """
        æŒ‰æ®µè½è¾¹ç•Œåˆ‡åˆ†æ–‡æœ¬

        Args:
            text: è¾“å…¥æ–‡æœ¬
            chunk_size: å—å¤§å°ï¼ˆæŒ‰å­—ç¬¦ä¼°ç®—ï¼Œçº¦ 1/3 tokenï¼‰

        Returns:
            æ–‡æœ¬å—åˆ—è¡¨
        """
        # ç®€å•å®ç°ï¼šæŒ‰æ®µè½åˆ‡åˆ†
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = []
        current_size = 0

        for para in paragraphs:
            para_size = len(para)

            if current_size + para_size > chunk_size * 3 and current_chunk:
                # å½“å‰å—å·²æ»¡ï¼Œä¿å­˜å¹¶å¼€å§‹æ–°å—
                chunks.append('\n\n'.join(current_chunk))
                current_chunk = [para]
                current_size = para_size
            else:
                current_chunk.append(para)
                current_size += para_size

        # ä¿å­˜æœ€åä¸€å—
        if current_chunk:
            chunks.append('\n\n'.join(current_chunk))

        return chunks
