"""
ReAct Agent

å¤šè½®å¯¹è¯Agentï¼Œæ”¯æŒThink-Act-Observeå¾ªç¯å’Œå·¥å…·è°ƒç”¨ã€‚

å‚è€ƒï¼šWebResummerçš„MultiTurnReactAgentå®ç°
è§„èŒƒæ–‡æ¡£ï¼šç¬¬7.1èŠ‚
"""

import logging
import json
import time
from typing import List, Dict, Any, Optional

logger = logging.getLogger(__name__)


class MultiTurnReactAgent:
    """
    å¤šè½®ReAct Agent

    ç‰¹ç‚¹ï¼š
    - Think-Act-Observeå¾ªç¯
    - å·¥å…·è°ƒç”¨ï¼ˆsearch, visit, deep_retrievalï¼‰
    - åœæ­¢æ¡ä»¶ï¼š<answer>æ ‡ç­¾
    - ä¸Šä¸‹æ–‡ç®¡ç†ï¼šTokenè®¡æ•° + è¶…é™å¤„ç†
    """

    def __init__(
        self,
        llm_client,
        tools: Dict[str, Any],
        system_message: str,
        max_iterations: int = 60,
        max_context_tokens: int = 32000
    ):
        """
        åˆå§‹åŒ–ReAct Agent

        Args:
            llm_client: LLMClient å®ä¾‹
            tools: å·¥å…·å­—å…¸ {tool_name: tool_instance}
            system_message: System Prompt
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            max_context_tokens: æœ€å¤§ä¸Šä¸‹æ–‡Tokenæ•°
        """
        self.llm_client = llm_client
        self.tools = tools
        self.system_message = system_message
        self.max_iterations = max_iterations
        self.max_context_tokens = max_context_tokens
        logger.info(
            f"MultiTurnReactAgentåˆå§‹åŒ–å®Œæˆ: "
            f"tools={list(tools.keys())}, "
            f"max_iterations={max_iterations}"
        )

    def run(self, question: str) -> Dict[str, Any]:
        """
        æ‰§è¡ŒReActå¾ªç¯

        Args:
            question: ç”¨æˆ·é—®é¢˜ï¼ˆå¯èƒ½å·²è¢«Adapterå¢å¼ºè¿‡ï¼‰

        Returns:
            æ‰§è¡Œç»“æœï¼š{
                "question": str,
                "prediction": str,  # æå–çš„ç­”æ¡ˆ
                "messages": List,   # å®Œæ•´è½¨è¿¹
                "termination": str, # ç»ˆæ­¢åŸå› 
                "iterations_used": int  # ä½¿ç”¨çš„è¿­ä»£æ¬¡æ•°
            }
        """
        logger.info(f"å¼€å§‹ReActå¾ªç¯: {question[:100]}...")
        print(f"\n{'='*80}")
        print(f"ğŸ¤– ReAct Agent å¼€å§‹æ‰§è¡Œ")
        print(f"{'='*80}")
        print(f"ä»»åŠ¡: {question[:200]}{'...' if len(question) > 200 else ''}")
        print(f"{'='*80}\n")

        # 1. åˆå§‹åŒ–
        messages = [
            {"role": "system", "content": self.system_message},
            {"role": "user", "content": question}
        ]
        full_trajectory = messages.copy()
        iterations_left = self.max_iterations

        # 2. ä¸»å¾ªç¯
        while iterations_left > 0:
            iterations_used = self.max_iterations - iterations_left
            iterations_left -= 1

            logger.debug(f"è¿­ä»£ {iterations_used + 1}/{self.max_iterations}")

            # 2.1 è°ƒç”¨LLM
            try:
                response = self.llm_client.call(messages)
            except Exception as e:
                logger.error(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
                return {
                    "question": question,
                    "prediction": "Error: LLM call failed",
                    "messages": full_trajectory,
                    "termination": "error",
                    "iterations_used": iterations_used
                }

            # 2.2 æ¸…ç†æ„å¤–çš„tool_responseæ ‡ç­¾
            if '<tool_response>' in response:
                pos = response.find('<tool_response>')
                response = response[:pos]
                logger.warning("æ¸…ç†äº†æ„å¤–çš„tool_responseæ ‡ç­¾")

            # 2.3 æ‰“å°LLMå“åº”ï¼ˆReActåŸå§‹è¾“å‡ºï¼‰
            print(f"\n{'='*80}")
            print(f"ğŸ“¤ ReAct Agent å“åº” (è¿­ä»£ {iterations_used + 1}):")
            print(f"{'='*80}")
            print(response.strip())
            print(f"{'='*80}\n")

            # 2.4 æ·»åŠ åˆ°æ¶ˆæ¯å†å²
            messages.append({"role": "assistant", "content": response.strip()})
            full_trajectory.append({"role": "assistant", "content": response.strip()})

            # 2.5 æ£€æŸ¥å·¥å…·è°ƒç”¨
            if '<tool_call>' in response and '</tool_call>' in response:
                logger.debug("æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨")
                tool_result = self._handle_tool_call(response)

                # æ‰“å°å·¥å…·å“åº”
                print(f"\n{'='*80}")
                print(f"ğŸ”§ å·¥å…·å“åº”:")
                print(f"{'='*80}")
                # æˆªæ–­è¿‡é•¿çš„å·¥å…·å“åº”ï¼ˆåªæ˜¾ç¤ºå‰2000ä¸ªå­—ç¬¦ï¼‰
                display_result = tool_result[:2000] + "...\n[å“åº”è¿‡é•¿ï¼Œå·²æˆªæ–­]" if len(tool_result) > 2000 else tool_result
                print(display_result)
                print(f"{'='*80}\n")

                messages.append({"role": "user", "content": tool_result})
                full_trajectory.append({"role": "user", "content": tool_result})

            # 2.6 æ£€æŸ¥ç­”æ¡ˆ
            elif '<answer>' in response and '</answer>' in response:
                answer = self._extract_answer(response)
                if answer:
                    print(f"\n{'='*80}")
                    print(f"âœ… ReAct Agent å®Œæˆ - è·å¾—æœ€ç»ˆç­”æ¡ˆ")
                    print(f"{'='*80}\n")
                    logger.info(f"è·å¾—ç­”æ¡ˆ: {answer[:100]}...")
                    return {
                        "question": question,
                        "prediction": answer,
                        "messages": full_trajectory,
                        "termination": "answer",
                        "iterations_used": iterations_used + 1
                    }

            # 2.7 Tokenè®¡æ•°å’Œä¸Šä¸‹æ–‡ç®¡ç†
            token_count = self.llm_client.count_tokens(str(messages))

            # 2.8 è¶…é™å¤„ç†
            if token_count > self.max_context_tokens:
                logger.warning(f"Tokenè¶…é™: {token_count} > {self.max_context_tokens}")

                # å¼ºåˆ¶è¦æ±‚ç”Ÿæˆç­”æ¡ˆ
                force_answer_msg = (
                    "You have now reached the maximum context length. "
                    "Please provide your final answer immediately using the <answer></answer> format."
                )
                messages.append({"role": "user", "content": force_answer_msg})
                full_trajectory.append({"role": "user", "content": force_answer_msg})

                response = self.llm_client.call(messages)
                messages.append({"role": "assistant", "content": response.strip()})
                full_trajectory.append({"role": "assistant", "content": response.strip()})

                answer = self._extract_answer(response)
                return {
                    "question": question,
                    "prediction": answer if answer else "No answer (token limit)",
                    "messages": full_trajectory,
                    "termination": "token_limit",
                    "iterations_used": iterations_used + 1
                }

        # 3. è¶…å‡ºè¿­ä»£æ¬¡æ•°
        logger.warning(f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°: {self.max_iterations}")
        answer = self._extract_answer(messages[-1]['content']) if messages else None
        return {
            "question": question,
            "prediction": answer if answer else "No answer found",
            "messages": full_trajectory,
            "termination": "max_iterations",
            "iterations_used": self.max_iterations
        }

    def _handle_tool_call(self, response: str) -> str:
        """
        å¤„ç†å·¥å…·è°ƒç”¨

        Args:
            response: LLMå“åº”ï¼ˆåŒ…å«<tool_call>æ ‡ç­¾ï¼‰

        Returns:
            å·¥å…·å“åº”ï¼ˆåŒ…å«<tool_response>æ ‡ç­¾ï¼‰
        """
        try:
            # 1. æå–JSON
            tool_call_str = response.split('<tool_call>')[1].split('</tool_call>')[0]
            tool_call = json.loads(tool_call_str.strip())

            # 2. æ‰§è¡Œå·¥å…·
            tool_name = tool_call.get('name', '')
            tool_args = tool_call.get('arguments', {})

            logger.info(f"æ‰§è¡Œå·¥å…·: {tool_name}, å‚æ•°: {tool_args}")

            if tool_name in self.tools:
                result = self.tools[tool_name].call(tool_args)
                logger.debug(f"å·¥å…·ç»“æœé•¿åº¦: {len(result)}")
            else:
                result = f"Error: Tool '{tool_name}' not found. Available tools: {list(self.tools.keys())}"
                logger.error(result)

            return f"<tool_response>{result}</tool_response>"

        except json.JSONDecodeError as e:
            error_msg = f"Error: Invalid JSON in tool call - {str(e)}"
            logger.error(error_msg)
            return f"<tool_response>{error_msg}</tool_response>"
        except Exception as e:
            error_msg = f"Error: Tool call failed - {str(e)}"
            logger.error(error_msg)
            return f"<tool_response>{error_msg}</tool_response>"

    def _extract_answer(self, response: str) -> Optional[str]:
        """
        æå–ç­”æ¡ˆ

        Args:
            response: LLMå“åº”

        Returns:
            æå–çš„ç­”æ¡ˆï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å› None
        """
        try:
            answer = response.split('<answer>')[1].split('</answer>')[0].strip()
            return answer
        except (IndexError, AttributeError):
            return None

    def __repr__(self) -> str:
        """è¿”å›Agentæ‘˜è¦"""
        return (
            f"MultiTurnReactAgent("
            f"tools={list(self.tools.keys())}, "
            f"max_iterations={self.max_iterations})"
        )
